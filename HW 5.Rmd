---
title: "HW 5"
author: "Collin Register"
date: "12/29/2023"
output:
  pdf_document: default
  html_document:
    number_sections: yes
  word_document: default
---

This homework is meant to give you practice in creating and defending a position with both statistical and philosophical evidence.  We have now extensively talked about the COMPAS ^[https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis] data set, the flaws in applying it but also its potential upside if its shortcomings can be overlooked.  We have also spent time in class verbally assessing positions both for an against applying this data set in real life.  In no more than two pages ^[knit to a pdf to ensure page count] take the persona of a statistical consultant advising a judge as to whether they should include the results of the COMPAS algorithm in their decision making process for granting parole.  First clearly articulate your position (whether the algorithm should be used or not) and then defend said position using both statistical and philosophical evidence.  Your paper will be grade both on the merits of its persuasive appeal but also the applicability of the statistical and philosohpical evidence cited.  



As a statistical consultant, it is my expert opinion that the COMPAS algorithm should not be used to predict repeat offenses and, therefore, parole sentencing. This algorithm should be steered clear of by the judicial system to maintain the principles of justice, fairness, and due process. Even alongside warnings and allowing for the algorithm to be used in conjunction with a judge's decision, the consequences of using COMPAS greatly outweigh the potential gains. The use of COMPAS should be avoided due to the lack of transparency of the algorithm, the violations of statistical measures of fairness, and the inherent moral concerns.

It is vital for judges to be fully informed in order to make just decisions and uphold due process. With that, even as an expert in the field of statistics, I am not able to fully inform you of the composition of the COMPAS algorithm. The algorithm is a black box, meaning what can be seen are only the inputs and outputs and not the process for achieving the outputs. This can be problematic because, without knowing how it arrives at the outputs, it could be using factors that should be protected or sensitive information. In addition, the creators of the algorithm, North Point, must keep the COMPAS black box to ensure that it remains for profit and copyrighted. It is important to know what predictive factors are used in determining recidivism to ensure fairness across the board.    

With this notion of fairness and predictive factors, it is necessary to note that, often times, zip codes are used as a proxy for race. This means that even if race were to be excluded, zip codes could still act as predictors because of the high association between race and zip code. With this, bias and discrimination would become a great concern. In COMPAS, discrimination is observed when statistical measures of fairness are violated in measures such as statistical parity, disparate impact, and equalized odds. Here, statistical parity measures whether groups receive similar outcomes, and a violation of this means discrimination is present. Disparate impact measures a similar notion and ensures that there are roughly the same classification rates in the two class labels across protected and unprotected sets. In the context of COMPAS, white is the protected label, and survived is the favorable outcome. Lastly, equalized odds consists of ensuring the absolute difference in false positive rates is comparable and that the absolute difference in true positive rates is comparable. The COMPAS algorithm performed poorly on these measures of fairness and still had a low overall accuracy rate. The overall accuracy rate was roughly 65%, which I deem to not be justifiable enough to be used in court.    

Lastly, predictive sentencing is not justifiable and is morally wrong. If this algorithm were to be used, it would treat people as an ends to a means by ignoring the circumstances of the offenders and their individual cases. The offenders would not be given the chance to do the right thing in the future, and they would be categorized based on characteristics. This would become a slippery slope, because it could lead to a feedback loop or force the bias to become real.  Also, if the algorithm were used in court, the potential for the judge to rely on only the algorithm and not consider it as a supplement is there. Similarly, if the algorithm were used in place of the judge, there would be no need for the judge to view the case and this would eliminate a human handling the case and considering the humanity of the judicial system. In addition, there is no accountability for the consequences of the algorithm wrongly classifying offenders. Are you willing to be held accountable if you implement this algorithm? That is a heavy burden to bear.    

To summarize, I would strongly advise against the use of COMPAS in your cases. The lack of transparency, poor performance within statistical measures of fairness, and moral considerations make this an easy suggestion to make.  I hope that you do not use the algorithm in considering parole and instead consider the individual facts of each case and ensure that the judicial system maintains your humanity.   


